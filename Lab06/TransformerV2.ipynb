{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731c0eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | CUDA: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Idx->Name: ['Condones', 'Cajas de condones', 'Esponjas', 'Botellas de Cloro', 'Placas madre']\n"
     ]
    }
   ],
   "source": [
    "# === Imports & config ===\n",
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Dispositivo y seeds\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"| CUDA:\", torch.cuda.is_available())\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Rutas (ajusta si usas otro nombre)\n",
    "DATA_DIR = Path(r\"dataset_dividido_v1\")     # contiene train/ val/ test/\n",
    "CLASSES_TXT = DATA_DIR / \"classes.txt\"      # opcional, solo informativo\n",
    "CLASS_IDX_JSON = DATA_DIR / \"class_indices.json\"  # opcional\n",
    "\n",
    "# Hiperparámetros\n",
    "IMG_SIZE   = 224\n",
    "PATCH_SIZE = 16\n",
    "EMBED_DIM  = 256\n",
    "DEPTH      = 6\n",
    "HEADS      = 8\n",
    "MLP_RATIO  = 4\n",
    "DROPOUT    = 0.1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 30\n",
    "LR         = 3e-4\n",
    "WD         = 0.05\n",
    "USE_AMP    = (device.type == \"cuda\")\n",
    "\n",
    "# Cargar nombres de clases (si existen)\n",
    "if CLASS_IDX_JSON.exists():\n",
    "    with open(CLASS_IDX_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        class_map = json.load(f)\n",
    "    idx2name = [class_map[str(i)] for i in range(len(class_map))]\n",
    "else:\n",
    "    idx2name = None\n",
    "print(\"Idx->Name:\", idx2name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be424156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases detectadas: ['Botellas de Cloro', 'Cajas de condones', 'Condones', 'Esponjas', 'Placas madre'] | NUM_CLASSES: 5\n"
     ]
    }
   ],
   "source": [
    "# === Data transforms ===\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.1,0.1,0.1,0.05)], p=0.3),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "ds = {\n",
    "    \"train\": datasets.ImageFolder(DATA_DIR / \"train\", transform=train_tfms),\n",
    "    \"val\":   datasets.ImageFolder(DATA_DIR / \"val\",   transform=eval_tfms),\n",
    "    \"test\":  datasets.ImageFolder(DATA_DIR / \"test\",  transform=eval_tfms),\n",
    "}\n",
    "NUM_CLASSES = len(ds[\"train\"].classes)\n",
    "print(\"Clases detectadas:\", ds[\"train\"].classes, \"| NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "dl = {\n",
    "    \"train\": DataLoader(ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True),\n",
    "    \"val\":   DataLoader(ds[\"val\"],   batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True),\n",
    "    \"test\":  DataLoader(ds[\"test\"],  batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyViT(\n",
      "  (patch): PatchEmbed(\n",
      "    (proj): Conv2d(3, 256, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n",
      "Parámetros entrenables: 4.99 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jhean\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === Tiny Vision Transformer===\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_ch=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size   = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid = (img_size // patch_size, img_size // patch_size)\n",
    "        self.num_patches = self.grid[0] * self.grid[1]\n",
    "        # Conv2d actúa como proyector de parches\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):  # x: [B,3,H,W]\n",
    "        x = self.proj(x)   # [B,embed_dim, H/P, W/P]\n",
    "        x = x.flatten(2).transpose(1,2)  # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_ch=3,\n",
    "                 embed_dim=256, depth=6, heads=8, mlp_ratio=4, num_classes=5, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(img_size, patch_size, in_ch, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch.num_patches, embed_dim))\n",
    "        self.pos_drop  = nn.Dropout(drop)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=heads,\n",
    "            dim_feedforward=int(embed_dim*mlp_ratio),\n",
    "            dropout=drop, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward(self, x):  # x: [B,3,224,224]\n",
    "        B = x.size(0)\n",
    "        x = self.patch(x)                          # [B, N, D]\n",
    "        cls = self.cls_token.expand(B, -1, -1)     # [B, 1, D]\n",
    "        x = torch.cat([cls, x], dim=1)             # [B, 1+N, D]\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]   # PE\n",
    "        x = self.pos_drop(x)\n",
    "        x = self.encoder(x)                        # [B, 1+N, D]\n",
    "        x = self.norm(x[:, 0])                     # CLS\n",
    "        return self.head(x)                        # [B, C]\n",
    "\n",
    "model = TinyViT(\n",
    "    img_size=IMG_SIZE, patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM, depth=DEPTH, heads=HEADS,\n",
    "    mlp_ratio=MLP_RATIO, num_classes=NUM_CLASSES, drop=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "sum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c38e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhean\\AppData\\Local\\Temp\\ipykernel_4996\\2325782819.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Entrenamiento TinyViT ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhean\\AppData\\Local\\Temp\\ipykernel_4996\\2325782819.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/30] train_loss=1.2510 acc=0.5031 | val_loss=1.1241 acc=0.5831\n",
      "[02/30] train_loss=1.1051 acc=0.5942 | val_loss=1.1298 acc=0.5455\n",
      "[03/30] train_loss=1.0346 acc=0.6249 | val_loss=1.0351 acc=0.6144\n",
      "[04/30] train_loss=0.9806 acc=0.6564 | val_loss=1.0132 acc=0.6332\n",
      "[05/30] train_loss=0.9342 acc=0.6746 | val_loss=0.9381 acc=0.6834\n",
      "[06/30] train_loss=0.8961 acc=0.6853 | val_loss=0.9056 acc=0.6928\n",
      "[07/30] train_loss=0.8506 acc=0.7163 | val_loss=0.9259 acc=0.6865\n",
      "[08/30] train_loss=0.8180 acc=0.7362 | val_loss=0.8582 acc=0.7116\n",
      "[09/30] train_loss=0.7888 acc=0.7503 | val_loss=0.8567 acc=0.7179\n",
      "[10/30] train_loss=0.7559 acc=0.7589 | val_loss=0.9232 acc=0.6803\n",
      "[11/30] train_loss=0.7372 acc=0.7763 | val_loss=0.7920 acc=0.7524\n",
      "[12/30] train_loss=0.7012 acc=0.7882 | val_loss=0.8462 acc=0.7179\n",
      "[13/30] train_loss=0.6744 acc=0.7996 | val_loss=0.8289 acc=0.7461\n",
      "[14/30] train_loss=0.6425 acc=0.8150 | val_loss=0.8535 acc=0.7524\n",
      "[15/30] train_loss=0.6067 acc=0.8291 | val_loss=0.8223 acc=0.7712\n",
      "[16/30] train_loss=0.5892 acc=0.8383 | val_loss=0.8144 acc=0.7524\n",
      "[17/30] train_loss=0.5519 acc=0.8584 | val_loss=0.8933 acc=0.7335\n",
      "[18/30] train_loss=0.5277 acc=0.8704 | val_loss=0.8615 acc=0.7210\n",
      "Early stopping 💡\n",
      "✓ Mejor modelo guardado en: best_tinyvit_residuos.pt\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR*0.05)\n",
    "\n",
    "# Nota: en tu torch 2.8, usa la versión CUDA para evitar el argumento device_type\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "BEST_PATH = Path(\"best_tinyvit_residuos.pt\")\n",
    "best_val_loss = float(\"inf\"); patience = 7; bad = 0\n",
    "\n",
    "def run_epoch(model, loader, train=True):\n",
    "    model.train(train)\n",
    "    tot_loss = 0.0; tot_corr = 0; tot = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(enabled=USE_AMP):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        bs = yb.size(0)\n",
    "        tot += bs\n",
    "        tot_loss += loss.item()*bs\n",
    "        tot_corr += (logits.argmax(1) == yb).sum().item()\n",
    "    return tot_loss/tot, tot_corr/tot\n",
    "\n",
    "print(\"== Entrenamiento TinyViT ==\")\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(model, dl[\"train\"], True)\n",
    "    va_loss, va_acc = run_epoch(model, dl[\"val\"],   False)\n",
    "    scheduler.step()\n",
    "    print(f\"[{ep:02d}/{EPOCHS}] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = va_loss; bad = 0\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping activao pa'\"); break\n",
    "\n",
    "print(\"✓ Mejor modelo guardado en:\", BEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b46e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy: 0.7292110874200426\n",
      "\n",
      "== Classification report ==\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Condones     0.6959    0.7744    0.7330       195\n",
      "Cajas de condones     0.7870    0.6351    0.7029       285\n",
      "         Esponjas     0.6343    0.6894    0.6607       161\n",
      "Botellas de Cloro     0.7358    0.7800    0.7573       150\n",
      "     Placas madre     0.7898    0.8435    0.8158       147\n",
      "\n",
      "         accuracy                         0.7292       938\n",
      "        macro avg     0.7286    0.7445    0.7339       938\n",
      "     weighted avg     0.7341    0.7292    0.7283       938\n",
      "\n",
      "Confusion matrix (rows true, cols pred):\n",
      " [[151  20  10  11   3]\n",
      " [ 39 181  35  12  18]\n",
      " [ 13  15 111  14   8]\n",
      " [ 10   7  12 117   4]\n",
      " [  4   7   7   5 124]]\n"
     ]
    }
   ],
   "source": [
    "# Cargar mejor estado\n",
    "state = torch.load(BEST_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "all_true, all_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl[\"test\"]:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        all_pred.append(logits.argmax(1).cpu().numpy())\n",
    "        all_true.append(yb.numpy())\n",
    "\n",
    "y_true = np.concatenate(all_true)\n",
    "y_pred = np.concatenate(all_pred)\n",
    "\n",
    "print(\"TEST accuracy:\", (y_true==y_pred).mean())\n",
    "names = idx2name if idx2name else [str(c) for c in range(NUM_CLASSES)]\n",
    "print(\"\\n== Classification report ==\")\n",
    "print(classification_report(y_true, y_pred, target_names=names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
    "print(\"Confusion matrix (rows true, cols pred):\\n\", cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
